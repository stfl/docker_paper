<!-- TODO make an Abstract title -->
<h1 id="introduction">Introduction</h1>
<!-- This section presents the various types of virtualization used for sharing physical resources; presents the open container standards; introduces the features of the Linux kernel which are used to implement Containers and references recent scientific work on this topic. -->
<!-- ## Typse of Virtualisation -->
<!-- Historically every service ran on its own physical server which lead to inefficient use of resources when the service did not utilize the full processing power of the server. -->
<p>In order to optimize the hardware utilization multiple services operate on the same physical server, sharing the hardware resources.</p>
<p>Several ways of sharing hardware resources are available which significantly differ in terms of isolation:</p>
<ul>
<li><p><strong>Shared hosting</strong> where services run alongside each other or even on the same physical server.</p></li>
<li><p><strong>chroot</strong> is an operating system (OS) kernel system call that changes the apparent root directory of a process to a subdirectory of the host.</p></li>
<li><p><strong>Full system virtualisation</strong> isolates the service by executing it inside its own virtual machine (VM) which fully emulates the entire hardware.</p></li>
<li><p><strong>Operating-system-level virtualisation</strong> or <strong>Containers</strong> are a set of OS kernel features to isolate individual processes.</p></li>
<li><p><strong>Hybrid VM containers</strong> is a new development of combining the benefits of isolation provided by VMs and the flexibility of containers.</p></li>
</ul>
<p>The first approach of shared hosting does not offer any significant isolation between the processes and does not offer additional protection against security threats. <!-- TODO introduce security threads --> Installing different versions of applications or libraries in a host system is typically difficult because common package managers like DPKG <span class="citation" data-cites="TeamsDpkgDebian">[1]</span> and RPM <span class="citation" data-cites="RpmOrg">[2]</span> do no allow the installation of multiple versions in the system.</p>
<p>When Using the system call chroot to create a so called jail, the effected process can only access files within this subdirectory. This allows isolation between processes on filesystem level and effortless installation of different library versions on the same system. Several ways to break out of such a jail exist <span class="citation" data-cites="BreakingOutChroot2016">[3]</span> and it is therefore not considered secure.</p>
<p>The development of full system virtualization revolutionized the hosting industry and allowed the evolution of cloud computing. The execution in individual virtual machines offers the maximum isolation. Inside the VM another full OS is running with all its system processes and services. VMs add a significant CPU and network performance overhead but because multiple instances can run on a single physical machine the resource is utilized more efficiently. The hypervisor which manages the VMs can segment the hardware resources and apply resource quotas to the individual VM to adapt the share of resources. <!-- migration --></p>
<p>In the last decade OS-level virtualization or containers as described in <span class="citation" data-cites="waltersComparisonVirtualizationTechnologies2008">[4]</span> have become increasingly popular. The container accesses the same OS kernel as the host instead of emulating the hardware. The isolation is only done through the segmentation features of the kernel and containers have less performance overhead than VMs and near-native startup time <span class="citation" data-cites="haleContainersPortableProductive2017">[5]</span>.</p>
<p>OS-level virtualization imposes a small overhead compared to native execution and offers better or equal performance than virtual machines <span class="citation" data-cites="felterUpdatedPerformanceComparison2015">[6]</span>. For example the CPU overhead introduced by Docker was measured to be 5% to 10% whereas the I/O overhead ranges from 10% to 30%.</p>
<p>One of the main reasons for containers is their lightweight design and simple deployment mechanisms through so-called orchestration software that manages multiple containers typically across multiple servers in multiple datacenters. Commonly services are started, stopped or moved from one server to another. The lightweight design of containers improves the performance of these operations. The OS kernel is considered a single-point-of failure and vulnerabilities in the kernel may allow malicious access from a container to the host or other containers <span class="citation" data-cites="linMeasurementStudyLinux2018">[7]</span>.</p>
<p>The recent development of so-calls hybrid VM containers combines virtual machines with containers by executing a container runtime in an extremely lightweight VM engine. Through complying to the open container standards, these containers can be integrated into flexible container orchestration software like Kubernetes <span class="citation" data-cites="UpdatesContainerIsolation">[8]</span>.</p>
<h2 id="open-container-standardization">Open Container Standardization</h2>
<p>The Open Container Initiative (OCI) <span class="citation" data-cites="OpenContainersInitiative">[9]</span> – formed under the Linux Foundation – and the Cloud Native Computing Foundation (CNCF) <span class="citation" data-cites="CloudNativeComputing">[10]</span> define the following <span class="citation" data-cites="DemystifyingContainerRuntimes">[11]</span>:</p>
<ul>
<li>Image Specification <span class="citation" data-cites="OCIImageFormat2019">[12]</span> defines the content of container images.</li>
<li>Runtime Specification (CRI) <span class="citation" data-cites="OCIRuntimeSpecification2019">[13]</span> describes the configuration, execution environment, and lifecycle of a container.</li>
<li>Container Network Interface (CNI) <span class="citation" data-cites="ContainerNetworkInterface2019">[14]</span> specifies how to configure network interfaces inside containers, which was standardized by the CNCF.</li>
</ul>
<h2 id="technical-background-of-containers-in-linux">Technical Background of Containers in Linux</h2>
<!-- This section presents the mechanisms and concepts of isolation provided by the Linux kernel to implement containers. -->
<p>The main mechanisms to implement container isolation are <em>namespaces</em> and control groups (<em>cgroups</em>). For security purposes secure computation (<em>seccomp</em>), capabilities and mandatory access control (MAC) mechanisms of the kernel are used. These mechanisms have been implemented into the Linux kernel over the years and strongly influenced by the growing container infrastructure.</p>
<p>Namespaces define what a process can see by creating a different view on the system <span class="citation" data-cites="NamespacesArticleIndex">[15]</span>. For example a process inside a container may see its process ID (PID) as 1 while it is mapped to a different PID outside of the container. Namespaces are for example also available for mount points, inter-process communication (IPC), network resources and users.</p>
<p>Cgroups define which resources a process can use <span class="citation" data-cites="UnderstandingNewControl">[16]</span>. With cgroups – amongst other things – the CPU or memory usage of a process can be limited. Other cgroups include device access, network and I/O throughput.</p>
<p>With seccomp a process can transition to a secure state where it can call only a limited subset of kernel system calls <span class="citation" data-cites="SeccompOverview2015">[17]</span>. The white-listing of system calls are done with filters that also consider the arguments of the system call.</p>
<p>MAC allows fine-grained control of permissions that are much more powerful than the traditional file-based permissions model. These rules are used with containers to additionally restrict the access of processes inside the container. There are various implementations for MAC and most commonly SELinux and AppArmor are used in conjunction with containers.</p>
<!-- # Recent Work -->
<!-- TODO: -->
<!-- Researches in [@xiePerformanceComparisonNative2018] compared Docker and rkt in Kubernetes and showed that Docker has a better CPU performance and rkt a better disk write performance. -->
<!-- Researchers in [@mavridisPerformanceOverheadStudy2017a] analyzed the impact of running Docker on top of KVM and showed a small overhead compared to execution directly in the VM. -->
<!-- On ARM architecture virtual machine show a better performance for I/O tasks due to the caching mechanisms in the hypervisor [@rahoKVMXenDocker2015]. -->
<!-- Several approaches to increase the container security are proposed in [@aguileraManagedContainersFramework2018],[@manuDockerContainerSecurity2016],[@MeasuringContainerSecurity]. They all focus on additional security layers to enforce a stronger segmentation between containers and the host. -->
<h1 id="container-frameworks">Container Frameworks</h1>
<p>This section presents various container frameworks for Linux in chronological order of their release which are all under active development. Containerization exist for Linux, FreeBSD, Solaris and Windows.</p>
<h2 id="openvz">OpenVZ</h2>
<p>OpenVZ <span class="citation" data-cites="OpenSourceContainerbased">[18]</span> was one of the first container solutions on Linux. It was released 2005 as part of Virtuozzo <span class="citation" data-cites="HyperconvergedInfrastructureSoftware">[19]</span> and is still under development although mainly use inside Virtuozzo. The full feature set of OpenVZ is only available with a custom kernel. Many of its features were merged upstream in the Linux kernel which significantly benefited the development of Linux container technology.</p>
<p>OpenVZ containers are so-called system containers, in contrast to application containers of Docker and similar solutions. System containers are closer to a full VM with a full init process, starting other system processes. An application container – like in Docker – typically starts only the actual process without any background processes running inside the container.</p>
<p>OpenVZ has a feature called checkpointing with CRIU <span class="citation" data-cites="CRIU">[20]</span>, which allows a container to stop during execution. This is a useful feature for live migration of containerized applications with slow startup time can be stored in the already started process. This is useful if an additional instance of an application has to be started up dynamically.</p>
<h2 id="lxclxd">LXC/LXD</h2>
<p>After the Linux mainline kernel added cgroups in the year 2007, LXC <span class="citation" data-cites="LinuxContainersLXC">[21]</span> was released in 2008 which was the first implementation of container management for Linux without requiring any patches. It is mainly based around the concepts of cgroups and namespaces.</p>
<p>LXC offers stateful snapstops and live migration by using CRIU from the OpenVZ project.</p>
<!-- Offers long-term releases with 5 years of bug fixes -->
<p>The Linux Container Daemon (LXD) <span class="citation" data-cites="LinuxContainersLXD">[22]</span> adds a layer on top of LXC. It is developed by Canonical and was first released in October 2015. In contrast to Docker, the focus is not on containerising a single application but having a full OS running inside.</p>
<p>LXD containers are system containers where the entry point for the container is not an application like in Docker but a full OS init system like systemd <span class="citation" data-cites="Systemd">[23]</span>. The focus of LXD is to have a full OS running inside. LXD aims to be used in a cloud environment as a replacement for VMs.</p>
<p>To manage the containers a secure application programming interface (API) to allow management to be done for example through cloud orchestration software like Canonical’s own Juju <span class="citation" data-cites="JujucharmsJuju">[24]</span>, OpenStack <span class="citation" data-cites="OpenStack">[25]</span> or Kubernetes <span class="citation" data-cites="Kubernetes">[26]</span>. LXD does not offer any network or storage management leaving that to the orchestration on top of it. On the other hand it offers life migration of containers to another host which is essential for dynamic load distribution in a cloud infrastructure.</p>
<h2 id="docker">Docker</h2>
<p>With the release of Docker <span class="citation" data-cites="merkelDockerLightweightLinux2014">[27]</span> in 2013 the container mechanisms had its break-through and Docker is still the most popular container solution. One of the philosophies behind Docker’s success was to hide complexity behind simplicity. This is especially true for the simplicity of the Command Line Interface (CLI) of Docker that significantly simplified initialisation and execution of the previously complex container management tasks.</p>
<p>Docker is not a standalone container runtime but consists of many software components with different purpose. Many of these components have been extracted in order to collaborate with other projects and support further development.</p>
<p>Docker uses a central daemon that handles the various running containerized processes and allows configuration by the user. Together with the configuration interface this is called the Docker Engine <span class="citation" data-cites="ederInfrastructureAgnosticApplication2017">[28]</span>.</p>
<!-- ![Docker Execution Driver [@DockerIntroducingExecution2014]](images/docker-execdriver-diagram.png){#fig:docker-exec-driver width=80%} -->
<p>Initially Docker used LXC as its container runtime and later developed its own runtime (libcontainer) which was later extracted and is now known as runC <span class="citation" data-cites="RunCCLITool2019">[29]</span>.</p>
<p>Docker had frequent API changes which was difficult to integrate in other systems which favor a stable API. This lead to the development of containerd as a standalone daemon with stable API which was donated as well to the OCI. Later, Docker engine also used containerd as shown in <strong>???</strong>. containerd uses runC to execute the container.</p>
<figure>
<img src="images/docker-engine-arch.png" alt="The Docker Engine architecture [30]" id="fig:docker-engine-arch" style="width:90.0%" /><figcaption>The Docker Engine architecture <span class="citation" data-cites="Docker11First2016">[30]</span></figcaption>
</figure>
<p>The docker architecture is designed around the concept that a single container only runs a single process. For example a container runs the web server, another container runs the database and yet another container runs a caching proxy in front of the service. This type of container is called an application container. Many of these application containers images are available as ready-to-use images that can be downloaded from a registry. Once the images is started it is referred to as a container.</p>
<p>Docker containers and images use a layered approach where each layer contains a change set to the layer underneath <span class="citation" data-cites="StorageDrivers2019">[31]</span>. As shown in <strong>???</strong> an image consists of various read-only layers and the execution of current container is done in an additional read-write layer on top of the image layers. With this layered concept if an image is executed multiple concurrent or sequential times the images layers are only stored on the system once. The same is true for image sub-layers where for example multiple layers can inherit a base layer provided by Ubuntu.</p>
<figure>
<img src="images/container-layers.jpg" alt="Docker Container Layers" id="fig:container-layers" style="width:80.0%" /><figcaption>Docker Container Layers</figcaption>
</figure>
<p>In Docker the life-time of a container matches the life-time of the process that is executed inside. If the process terminates, the container terminates with it. If a Docker container is deleted all changes made inside the container are deleted as well. In order to store the changes in a container a snapshot of the current state can be taken a stored as a new image that can again be started up at that state. Furthermore, directories from outside the container can be mapped inside the container to store consistent changes outside the container.</p>
<p>Docker images can be built using a Dockerfile that starts off at a specified image and specifies instructions to be executed inside the container before the resulting image is saved. With this method images can be produced automatically at for example a new release of the application packaged inside the Docker image.</p>
<p>Docker supports multiple network drivers <span class="citation" data-cites="DockerNetworkingDocumentation2019">[32]</span> which allow networking interfaces to act as <em>bridge</em> where the virtual docker network is separated from the host network; <em>host</em> where the container has direct access to the hosts network interface; <em>overlay</em> where a virtual network is created between multiple hosts that run the Docker daemon; <em>macvlan</em> where each container gets a unique mac address on the host’s network and Docker even allows third-party networking plugins to be used.</p>
<!-- Docker is available in also available in OpenStack, CloudFoundry and  -->
<!-- Docker and later containerd was the underlying container runtime for Kubernetes.  -->
<h2 id="runc">runC</h2>
<p>runC <span class="citation" data-cites="RunCCLITool2019">[29]</span> is a lightweight container runtime without a daemon which originated as a software component of Docker and is now developed by the OCI. It gives the user full control over the execution of the container which allowed the development of other solutions on top of containers that use runC as their container runtime.</p>
<p>An example of software on top of runC is CRI-O <span class="citation" data-cites="Crio">[33]</span> which is a lightweight and simple container runtime developed by Red Hat as a minimal container runtime for Kubernetes which was released in October 2017. “CRI-O is designed to be simpler than other solutions, following the Unix philosophy of doing one thing and doing it well, with reusable components.” <span class="citation" data-cites="KubeConCloudNativeConNorth">[34]</span>. The goal for CRI-O is to have maximum compatibility with Kubernetes and provide a stable interface.</p>
<p>Another example is Garden <span class="citation" data-cites="GardenCloudFoundry">[35]</span>, which is the containerization layer in the CloudFoundry orchestration software <span class="citation" data-cites="CloudFoundryOpen">[36]</span>. The Garden project moved from LXC to runC as their container back-end on Linux.</p>
<h2 id="rkt">rkt</h2>
<p>rkt <span class="citation" data-cites="CoreOSRkt">[37]</span> (pronounced “rocket”) is an open-source container engine developed by CoreOS project. rkt took security into consideration from the early design. rkt offers encryption and signing of images <span class="citation" data-cites="CoreOSRktImage">[38]</span>. In addition to rkt’s native image format it can execute Docker images.</p>
<p>Same as Docker rkt focuses on application containers where the ideally only a single process is executed inside a container. rkt does not have a centralized daemon for configuration but integrates in systemd to start containers directly under the Linux init process. This eliminates the single point-of-failure of having a centralized daemon like Docker. <strong>???</strong> shows the execution hierarchy of rkt compared to Docker.</p>
<figure>
<img src="images/docker_rkt_exec_structure.gif" alt="Execution hierarchy Docker vs. rkt [39]" id="fig:rkt-exec-hierarchy" style="width:60.0%" /><figcaption>Execution hierarchy Docker vs. rkt <span class="citation" data-cites="xie2017performance">[39]</span></figcaption>
</figure>
<p>rkt developed and uses the App Container standard appC <span class="citation" data-cites="AppContainerSpecification2019">[40]</span> which was later included in the OCI standards. The project compares in <span class="citation" data-cites="AppContainerSpecification2019README">[41]</span> the appC standards to the OCI standard as follows: “<em>The App Container Image format (ACI) is maps more or less directly to the OCI Image Format Specification, with the exception of signing and dependencies. The App Container Executor (ACE) specification is related conceptually to the OCI Runtime Specification, with the notable distinctions that the latter does not support pods and generally operates at a lower level of specification. App Container Image Discovery does not yet have an equivalent specification in the OCI project.</em>”</p>
<p>appC uses the concept of pods as the basic unit of execution. “<em>A pod is a grouping of one or more app images (ACIs), with some additional metadata optionally applied to the pod as a whole.</em>”<span class="citation" data-cites="CoreOSRktApp">[42]</span> This allows configuration of — for example — resource constraints and networking for multiple containers from a single point. The concept of pods in rkt is identical to the concept of pods in Kubernetes <span class="citation" data-cites="WhatKubernetesUsers">[43]</span> which allows a good integration and rkt became the first non-Docker runtime to be supported in Kubernetes <span class="citation" data-cites="RktnetesBringsRkt">[44]</span>.</p>
<figure>
<img src="images/rkt-execution-flow.png" alt="rkt execution workflow [45]" id="fig:rkt-exec-flow" style="width:50.0%" /><figcaption>rkt execution workflow <span class="citation" data-cites="CoreOSRktArchitecture">[45]</span></figcaption>
</figure>
<p>The rkt execution workflow as shown in <strong>???</strong> describes three stages. After the pod is started from various sources like the command line, systemd or Kubernetes the container runtime is set up in stage 1. Different execution environments are available in rkt. The default options are:</p>
<ul>
<li><strong>systemd-nspawnd</strong> the default behavior that uses the cgroups and namespaces invoked from systemd.</li>
<li><strong>fly</strong> executes the process in a chroot without further isolation</li>
<li><strong>KVM</strong> uses a fully virtualized environment</li>
</ul>
<p>Other options for stage 1 are available as plugins. In stage 2 the process inside the container is launched.</p>
<p>rkt is currently changing its internal architecture to be compliant to the OCI standard <span class="citation" data-cites="WhatKubernetesUsers">[43]</span> and is currently developing the integration of runC as a stage 2 execution environment. The current status rkt’s OCI transition can be seen at <span class="citation" data-cites="CoreOSRktOCI2019">[46]</span>.</p>
<p>To build rkt images scripts are available that are similar to Dockerfiles.</p>
<p>rkt uses systemd-journald for logging which integrates directly in the logging infrastructure of the host system.</p>
<p>Although, rkt may arguably have a better architecture than other solutions it seems that developers have stopped actively developing rkt when CoreOS was bought by Red Hat in Spring 2018. There is no official announcement but the last GitHub commit on rkt was in May 2018.</p>
<h2 id="kata-containers">Kata Containers</h2>
<p>Kata containers <span class="citation" data-cites="KataContainersSpeed">[47]</span> is an OCI-compliant hybrid VM container runtime that executes containers within virtual machines released in 2018.</p>
<p>Kata Container originated from merging of the two projects Hyper runV <span class="citation" data-cites="HyperMakeVM">[48]</span> and Intel Clear Containers <span class="citation" data-cites="IntelClearContainers2019">[49]</span> which focused on securing the runtime environment that containers execute in by executing the container inside a Kernel Virtual Machine (KVM) runtime <span class="citation" data-cites="IntelHyperSh2017">[50]</span>.</p>
<p>Kata containers is developed under the OpenStack foundation and provide a virtualized isolation layer to make container execution more secure <span class="citation" data-cites="OpenStackBoostsContainer2018">[51]</span>. The project is supported by several companies to accelerate development and hardware support.</p>
<p>Kata Containers can be integrated into Docker and containerd and into Kubernetes with Kata containers OCI-compatable runtime or directly into the CRI layer with Frakti <span class="citation" data-cites="HypervisorbasedContainerRuntime2019">[52]</span>. <strong>???</strong> shows the different integration options of Kata containers into Kubernetes.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <strong>???</strong> shows how Kata containers can be executed in Kubernetes under CRI-O alongside other OCI compliant containers.</p>
<figure>
<img src="images/kubernetesandkatacontainers.jpg" alt="Kata Container intergration into Kubernets [53]" id="fig:kata-kubernets" style="width:90.0%" /><figcaption>Kata Container intergration into Kubernets <span class="citation" data-cites="KataContainersWhy">[53]</span></figcaption>
</figure>
<figure>
<img src="images/cri-o-kata.jpg" alt="Kata Container intergration into Kubernets with CRI-O [53]" id="fig:cri-o-kata" style="width:80.0%" /><figcaption>Kata Container intergration into Kubernets with CRI-O <span class="citation" data-cites="KataContainersWhy">[53]</span></figcaption>
</figure>
<p>Kata Containers uses a minimal version of QEMU for virtualization. For faster start-up time of the VM container other virtualization engines have been added. Noticeably, Firecracker MicroVM <span class="citation" data-cites="FirecrackerMicroVM">[54]</span> is available with Kata Containers as of version 1.5<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> <span class="citation" data-cites="KataContainersDocumentation2019">[55]</span>. Firecracker MicroVM is a minimalistic virtual machine developed by Amazon as a faster replacement for QEMU. Firecracker only emulates 4 devices for networking, block devices, serial console and a 1-button keyboard controller used only to stop the microVM. This minimalistic design enables a startup time of less than 125ms.</p>
<h2 id="other-frameworks">Other Frameworks</h2>
<p>There are many other solutions available and the following few are mentioned only for reference and an interested reader might continue there.</p>
<ul>
<li><p>systemd-nspawnd <span class="citation" data-cites="Systemdnspawn">[56]</span> is part of systemd and allows the creation of a container directly from systemd</p></li>
<li><p>gVisor <span class="citation" data-cites="ContainerRuntimeSandbox2019">[57]</span> is an attempt to secure containers by providing a user space kernel abstraction that processes the system calls used by the container process.</p></li>
<li><p>Nabla containers <span class="citation" data-cites="NablaContainersNew">[58]</span> tries to secure containers by restricting the kernel system calls with a strict seccomp profile that allows only 7 system calls.</p></li>
<li><p>Singularity <span class="citation" data-cites="Singularity">[59]</span> is a container framework intended for scientific computing where operations are executed on many notes in a high performance cluster.</p></li>
</ul>
<h1 id="comparison">Comparison</h1>
<p>This section compares various characteristics of the previously introduced container frameworks and the availability of their frameworks in different orchestrator solutions.</p>
<table>
<caption>Comparison of Container framework characteristics {#tbl:comp_char}</caption>
<colgroup>
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th>Framework</th>
<th>Release</th>
<th>Isolation</th>
<th>Sys/App container</th>
<th>central daemon</th>
<th>live migration</th>
<th>resource quotas</th>
<th>OCI compliant</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>OpenVZ</td>
<td>2005</td>
<td>Kernel</td>
<td>System</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="even">
<td>LXC</td>
<td>2008</td>
<td>Kernel</td>
<td>System</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="odd">
<td>LXD</td>
<td>2016</td>
<td>Kernel</td>
<td>System</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr class="even">
<td>Docker</td>
<td>2014</td>
<td>Kernel</td>
<td>App</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="odd">
<td>runC</td>
<td>2016<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></td>
<td>Kernel</td>
<td>App</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="even">
<td>rkt</td>
<td>2016</td>
<td>Kernel</td>
<td>App</td>
<td>no</td>
<td>no</td>
<td>yes</td>
<td>partial</td>
</tr>
<tr class="odd">
<td>Kata Containers</td>
<td>2018</td>
<td>VM</td>
<td>App</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
</tbody>
</table>
<p><strong>???</strong> illustrates some characteristics of the presented container frameworks. It shows the release year of the framework and the underlying isolation strategy of the framework where traditional container frameworks offer isolation through Linux Kernel mechanisms and Kata Containers offers isolation through full system virtualization. Some containers are designed as system containers while other – newer frameworks, starting with Docker – are application containers. The only two container frameworks with a centralized daemon are Docker and LXD, whereas LXD is essentially the daemon for LXC. All framework but rkt support live migration while the container solutions use CRIU and Kata containers the snapshotting feature of the underlying virtualization engine. All container frameworks all the specification of CPU, network, I/O and memory quotas and are therefore not listed separately. Not all frameworks are OCI compliant because OCI standardization focuses on application containers. Docker which uses runC internally is OCI compliant. rkt does not fully comply to the OCI standard and Kata containers was developed after the standardization and is one of the first container frameworks that focus on OCI compliance from the start.</p>
<table style="width:100%;">
<caption>Availability of Container Framework in various Orchestrators {#tbl:comp_orchestrator}</caption>
<colgroup>
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th>Orchestrator→ Container ↓</th>
<th>Kubernetes</th>
<th>OpenStack</th>
<th>CloudFoundry</th>
<th>Mesos<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></th>
<th>Virtuozzo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>OpenVZ</td>
<td>no<span class="math inline"><em></em><sup>3</sup></span></td>
<td>no<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>LXC/LXD</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
<tr class="odd">
<td>Docker</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></td>
</tr>
<tr class="even">
<td>runC</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
<td>no</td>
</tr>
<tr class="odd">
<td>rkt</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
<tr class="even">
<td>Kata</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
</tbody>
</table>
<p>An interesting distinction between container frameworks is their integration into orchestration software on top of the container. OpenVZ was developed inside Virtuozzo with a separate Linux kernel and some of its features are not available upstream. It is therefore only available in Virtuozzo. On top of Virtuozzo it is possible to deploy Kubernetes and OpenStack and it is possible to run Docker inside OpenVZ which makes it available in Virtuozzo. All other frameworks are directly or with shim layers available in Kubernetes and OpenStack. CloudFoundy supports Docker and integrated runC into there containerization engine. Apache Mesos <span class="citation" data-cites="ApacheMesos">[60]</span> offers support for Docker alongside their own containerization implementation.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Docker made containers popular by significantly simplifying container management through their easy-to-use CLI and API and large selection of readily available images. Docker’s central daemon containerd is build modular to facility integration into orchestrator software and allows the underlaying execution runtime to be replaces with another runtime. For example Kata containers even with Firecracker MicroVM can directly be used with the same Docker API.</p>
<p>Through the standardization the industry has agreed on a single set of standards which was mainly pushed by Docker and the standardized standalone runtime runC is directly integrated into other solutions that need to have full low-level control of the container.</p>
<p>This makes Docker for simple use the best choice when directly interacting with the containers and runC the best options when containers need to be integrated into other solutions with which the user interacts with.</p>
<p>Most container in the cloud are executed inside a VM due to security concerns of the operators. This fact and its strong focus on integrability makes Kata Containers – which is currently still under heavy development – a very promising alternative to eliminating this layer of virtualization.</p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-TeamsDpkgDebian">
<p>[1] “Teams/Dpkg - Debian Wiki.” <a href="https://wiki.debian.org/Teams/Dpkg" class="uri">https://wiki.debian.org/Teams/Dpkg</a>. </p>
</div>
<div id="ref-RpmOrg">
<p>[2] “Rpm.Org.” <a href="https://rpm.org/" class="uri">https://rpm.org/</a>. </p>
</div>
<div id="ref-BreakingOutChroot2016">
<p>[3] “Breaking out of a chroot() padded cell.” <a href="https://web.archive.org/web/20160127150916/http://www.bpfh.net/simes/computing/chroot-break.html" class="uri">https://web.archive.org/web/20160127150916/http://www.bpfh.net/simes/computing/chroot-break.html</a>, Jan-2016. </p>
</div>
<div id="ref-waltersComparisonVirtualizationTechnologies2008">
<p>[4] J. P. Walters, V. Chaudhary, M. Cha, S. Guercio Jr, and S. Gallo, “A comparison of virtualization technologies for HPC,” in <em>Advanced Information Networking and Applications, 2008. AINA 2008. 22nd International Conference on</em>, 2008, pp. 861–868. </p>
</div>
<div id="ref-haleContainersPortableProductive2017">
<p>[5] J. S. Hale, L. Li, C. N. Richardson, and G. N. Wells, “Containers for Portable, Productive, and Performant Scientific Computing,” <em>Computing in Science Engineering</em>, vol. 19, no. 6, pp. 40–50, Nov. 2017. </p>
</div>
<div id="ref-felterUpdatedPerformanceComparison2015">
<p>[6] W. Felter, A. Ferreira, R. Rajamony, and J. Rubio, “An updated performance comparison of virtual machines and linux containers,” in <em>Performance Analysis of Systems and Software (ISPASS), 2015 IEEE International Symposium On</em>, 2015, pp. 171–172. </p>
</div>
<div id="ref-linMeasurementStudyLinux2018">
<p>[7] X. Lin, L. Lei, Y. Wang, J. Jing, K. Sun, and Q. Zhou, “A Measurement Study on Linux Container Security: Attacks and Countermeasures,” in <em>Proceedings of the 34th Annual Computer Security Applications Conference</em>, 2018, pp. 418–429. </p>
</div>
<div id="ref-UpdatesContainerIsolation">
<p>[8] “Updates in container isolation [LWN.Net].” <a href="https://lwn.net/Articles/754433/" class="uri">https://lwn.net/Articles/754433/</a>. </p>
</div>
<div id="ref-OpenContainersInitiative">
<p>[9] “Open Containers Initiative.” <a href="https://www.opencontainers.org/" class="uri">https://www.opencontainers.org/</a>. </p>
</div>
<div id="ref-CloudNativeComputing">
<p>[10] “Cloud Native Computing Foundation,” <em>Cloud Native Computing Foundation</em>. <a href="https://www.cncf.io/" class="uri">https://www.cncf.io/</a>. </p>
</div>
<div id="ref-DemystifyingContainerRuntimes">
<p>[11] “Demystifying container runtimes.” <a href="https://lwn.net/Articles/741897/" class="uri">https://lwn.net/Articles/741897/</a>. </p>
</div>
<div id="ref-OCIImageFormat2019">
<p>[12] “OCI Image Format.” <a href="https://github.com/opencontainers/image-spec" class="uri">https://github.com/opencontainers/image-spec</a>, Jan-2019. </p>
</div>
<div id="ref-OCIRuntimeSpecification2019">
<p>[13] “OCI Runtime Specification.” <a href="https://github.com/opencontainers/runtime-spec" class="uri">https://github.com/opencontainers/runtime-spec</a>, Jan-2019. </p>
</div>
<div id="ref-ContainerNetworkInterface2019">
<p>[14] “Container Network Interface: Networking for Linux containers.” <a href="https://github.com/containernetworking/cni" class="uri">https://github.com/containernetworking/cni</a>, Jan-2019. </p>
</div>
<div id="ref-NamespacesArticleIndex">
<p>[15] “Namespaces article index [LWN.Net].” <a href="https://lwn.net/Articles/766124/" class="uri">https://lwn.net/Articles/766124/</a>. </p>
</div>
<div id="ref-UnderstandingNewControl">
<p>[16] “Understanding the new control groups API.” <a href="https://lwn.net/Articles/679786/" class="uri">https://lwn.net/Articles/679786/</a>. </p>
</div>
<div id="ref-SeccompOverview2015">
<p>[17] “A seccomp overview.” <a href="https://lwn.net/Articles/656307/" class="uri">https://lwn.net/Articles/656307/</a>, Sep-2015. </p>
</div>
<div id="ref-OpenSourceContainerbased">
<p>[18] “Open source container-based virtualization for Linux.” <em>OpenVz</em>. <a href="https://openvz.org/" class="uri">https://openvz.org/</a>. </p>
</div>
<div id="ref-HyperconvergedInfrastructureSoftware">
<p>[19] “Hyperconverged Infrastructure Software Provider.” <a href="https://www.virtuozzo.com/" class="uri">https://www.virtuozzo.com/</a>. </p>
</div>
<div id="ref-CRIU">
<p>[20] “CRIU.” <a href="https://criu.org/Main_Page" class="uri">https://criu.org/Main_Page</a>. </p>
</div>
<div id="ref-LinuxContainersLXC">
<p>[21] “Linux Containers - LXC - Introduction.” <a href="https://linuxcontainers.org/lxc/introduction/" class="uri">https://linuxcontainers.org/lxc/introduction/</a>. </p>
</div>
<div id="ref-LinuxContainersLXD">
<p>[22] “Linux Containers - LXD - Introduction.” <a href="https://linuxcontainers.org/lxd/" class="uri">https://linuxcontainers.org/lxd/</a>. </p>
</div>
<div id="ref-Systemd">
<p>[23] “Systemd.” <a href="https://freedesktop.org/wiki/Software/systemd/" class="uri">https://freedesktop.org/wiki/Software/systemd/</a>. </p>
</div>
<div id="ref-JujucharmsJuju">
<p>[24] “Jujucharms | Juju.” <a href="https://jujucharms.com/" class="uri">https://jujucharms.com/</a>. </p>
</div>
<div id="ref-OpenStack">
<p>[25] “OpenStack: Build the future of Open Infrastructure.” <a href="https://www.openstack.org/" class="uri">https://www.openstack.org/</a>. </p>
</div>
<div id="ref-Kubernetes">
<p>[26] “Kubernetes: Production-Grade Container Orchestration.” <a href="https://kubernetes.io/" class="uri">https://kubernetes.io/</a>. </p>
</div>
<div id="ref-merkelDockerLightweightLinux2014">
<p>[27] D. Merkel, “Docker: Lightweight Linux Containers for Consistent Development and Deployment,” <em>Linux J.</em>, vol. 2014, no. 239, Mar. 2014. </p>
</div>
<div id="ref-ederInfrastructureAgnosticApplication2017">
<p>[28] P. Eder, “An infrastructure agnostic application deployment framework for the Internet of things,” PhD thesis, Wien, 2017. </p>
</div>
<div id="ref-RunCCLITool2019">
<p>[29] “runC: CLI tool for spawning and running containers according to the OCI specification: Opencontainers/runc.” <a href="https://github.com/opencontainers/runc" class="uri">https://github.com/opencontainers/runc</a>, Jan-2019. </p>
</div>
<div id="ref-Docker11First2016">
<p>[30] “Docker 1.11: The first runtime built on containerd and based on OCI technology,” <em>Docker Blog</em>. <a href="https://blog.docker.com/2016/04/docker-engine-1-11-runc/" class="uri">https://blog.docker.com/2016/04/docker-engine-1-11-runc/</a>, Apr-2016. </p>
</div>
<div id="ref-StorageDrivers2019">
<p>[31] “About storage drivers,” <em>Docker Documentation</em>. <a href="https://docs.docker.com/storage/storagedriver/" class="uri">https://docs.docker.com/storage/storagedriver/</a>, Jan-2019. </p>
</div>
<div id="ref-DockerNetworkingDocumentation2019">
<p>[32] “Docker Networking Documentation,” <em>Docker Documentation</em>. <a href="https://docs.docker.com/network/" class="uri">https://docs.docker.com/network/</a>, Jan-2019. </p>
</div>
<div id="ref-Crio">
<p>[33] “Cri-o.” <a href="https://cri-o.io/" class="uri">https://cri-o.io/</a>. </p>
</div>
<div id="ref-KubeConCloudNativeConNorth">
<p>[34] “KubeCon + CloudNativeCon North America 2017: CRI-O: All the Runtime Kubernetes Needs,...” <a href="https://kccncna17.sched.com/event/CU6T/cri-o-all-the-runtime-kubernetes-needs-and-nothing-more-mrunal-patel-red-hat" class="uri">https://kccncna17.sched.com/event/CU6T/cri-o-all-the-runtime-kubernetes-needs-and-nothing-more-mrunal-patel-red-hat</a>. </p>
</div>
<div id="ref-GardenCloudFoundry">
<p>[35] “Garden | Cloud Foundry.” <a href="https://docs.cloudfoundry.org/concepts/architecture/garden.html" class="uri">https://docs.cloudfoundry.org/concepts/architecture/garden.html</a>. </p>
</div>
<div id="ref-CloudFoundryOpen">
<p>[36] “Cloud Foundry: Open Source Cloud Application Platform,” <em>Cloud Foundry</em>. <a href="https://www.cloudfoundry.org/" class="uri">https://www.cloudfoundry.org/</a>. </p>
</div>
<div id="ref-CoreOSRkt">
<p>[37] “CoreOS rkt.” <a href="https://coreos.com/rkt/" class="uri">https://coreos.com/rkt/</a>. </p>
</div>
<div id="ref-CoreOSRktImage">
<p>[38] “CoreOS rkt Image Signing and Verification Guide.” <a href="https://coreos.com/rkt/docs/latest/signing-and-verification-guide.html" class="uri">https://coreos.com/rkt/docs/latest/signing-and-verification-guide.html</a>. </p>
</div>
<div id="ref-xie2017performance">
<p>[39] X.-L. Xie, P. Wang, and Q. Wang, “The performance analysis of Docker and rkt based on Kubernetes,” in <em>2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)</em>, 2017, pp. 2137–2141. </p>
</div>
<div id="ref-AppContainerSpecification2019">
<p>[40] “App Container Specification and Tooling.” <a href="https://github.com/appc/spec/blob/master/SPEC.md" class="uri">https://github.com/appc/spec/blob/master/SPEC.md</a>, Jan-2019. </p>
</div>
<div id="ref-AppContainerSpecification2019README">
<p>[41] “App Container Specification and Tooling. README.” <a href="https://github.com/appc/spec/blob/master/README.md" class="uri">https://github.com/appc/spec/blob/master/README.md</a>, Jan-2019. </p>
</div>
<div id="ref-CoreOSRktApp">
<p>[42] “CoreOS rkt App Container Basics.” <a href="https://coreos.com/rkt/docs/latest/app-container.html" class="uri">https://coreos.com/rkt/docs/latest/app-container.html</a>. </p>
</div>
<div id="ref-WhatKubernetesUsers">
<p>[43] “What Kubernetes users should know about the rkt container engine | CoreOS.” <a href="https://coreos.com/blog/rkt-and-kubernetes.html" class="uri">https://coreos.com/blog/rkt-and-kubernetes.html</a>. </p>
</div>
<div id="ref-RktnetesBringsRkt">
<p>[44] “Rktnetes brings rkt container engine to Kubernetes.” <a href="https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/" class="uri">https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/</a>. </p>
</div>
<div id="ref-CoreOSRktArchitecture">
<p>[45] “CoreOS rkt Architecture Documentation.” <a href="https://coreos.com/rkt/docs/latest/devel/architecture.html" class="uri">https://coreos.com/rkt/docs/latest/devel/architecture.html</a>. </p>
</div>
<div id="ref-CoreOSRktOCI2019">
<p>[46] “CoreOS rkt OCI native support | current project state.” <a href="https://github.com/rkt/rkt/projects/4" class="uri">https://github.com/rkt/rkt/projects/4</a>, Jan-2019. </p>
</div>
<div id="ref-KataContainersSpeed">
<p>[47] “Kata Containers - The speed of containers, the security of VMs.” <a href="https://katacontainers.io/" class="uri">https://katacontainers.io/</a>. </p>
</div>
<div id="ref-HyperMakeVM">
<p>[48] “Hyper - Make VM run like Container.” <a href="https://www.hypercontainer.io/" class="uri">https://www.hypercontainer.io/</a>. </p>
</div>
<div id="ref-IntelClearContainers2019">
<p>[49] “Intel Clear Containers.” <a href="https://github.com/clearcontainers/runtime" class="uri">https://github.com/clearcontainers/runtime</a>, Jan-2019. </p>
</div>
<div id="ref-IntelHyperSh2017">
<p>[50] “Intel, Hyper.Sh merge tech into Kata Containers,” <em>ICT Monitor Worldwide</em>, Dec. 2017. </p>
</div>
<div id="ref-OpenStackBoostsContainer2018">
<p>[51] “OpenStack Boosts Container Security With Kata Containers 1.0,” <em>ICT Monitor Worldwide</em>, May 2018. </p>
</div>
<div id="ref-HypervisorbasedContainerRuntime2019">
<p>[52] “The hypervisor-based container runtime for Kubernetes.” <a href="https://github.com/kubernetes/frakti" class="uri">https://github.com/kubernetes/frakti</a>, Jan-2019. </p>
</div>
<div id="ref-KataContainersWhy">
<p>[53] “Kata Containers - Why Kata Containers doesn’t replace Kubernetes: A Kata Containers explainer.” <a href="https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/" class="uri">https://katacontainers.io/posts/why-kata-containers-doesnt-replace-kubernetes/</a>. </p>
</div>
<div id="ref-FirecrackerMicroVM">
<p>[54] “Firecracker MicroVM.” <a href="https://firecracker-microvm.github.io/" class="uri">https://firecracker-microvm.github.io/</a>. </p>
</div>
<div id="ref-KataContainersDocumentation2019">
<p>[55] “Kata Containers documentation | Initial release of Kata Containers with Firecracker support.” <a href="https://github.com/kata-containers/documentation/wiki/Initial-release-of-Kata-Containers-with-Firecracker-support" class="uri">https://github.com/kata-containers/documentation/wiki/Initial-release-of-Kata-Containers-with-Firecracker-support</a>, Jan-2019. </p>
</div>
<div id="ref-Systemdnspawn">
<p>[56] “Systemd-nspawn.” <a href="https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html" class="uri">https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html</a>. </p>
</div>
<div id="ref-ContainerRuntimeSandbox2019">
<p>[57] “Container Runtime Sandbox. Contribute to google/gvisor development by creating an account on GitHub.” <a href="https://github.com/google/gvisor" class="uri">https://github.com/google/gvisor</a>, Jan-2019. </p>
</div>
<div id="ref-NablaContainersNew">
<p>[58] “Nabla containers: A new approach to container isolation,” <em>Nabla Containers</em>. <a href="https://nabla-containers.github.io/" class="uri">https://nabla-containers.github.io/</a>. </p>
</div>
<div id="ref-Singularity">
<p>[59] “Singularity,” <em>Sylabs.io</em>. <a href="https://www.sylabs.io/singularity/" class="uri">https://www.sylabs.io/singularity/</a>. </p>
</div>
<div id="ref-ApacheMesos">
<p>[60] “Apache Mesos,” <em>Apache Mesos</em>. <a href="http://mesos.apache.org/" class="uri">http://mesos.apache.org/</a>. </p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is a good representation of the current development trend towards open standardization compliance on all layers.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>released Jan 22. 2019<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Stable version 1.0 has not yet been released.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Also has its own containerization engine.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>On top of Virtuozzo<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Inside OpenVZ containers<a href="#fnref6" class="footnote-back">↩</a></p></li>
</ol>
</section>
